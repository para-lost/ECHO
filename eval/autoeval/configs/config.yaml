gpt_kwargs:
  api_cls: "AsyncOpenAI"

qwen_kwargs:
  model: qwen/qwen2.5-vl-32b-instruct
  max_concurrency: 4
  call_kwargs:
    temperature: 0.0

gemini_kwargs:
  model: google/gemini-2.0-flash-001
  max_concurrency: 8
  call_kwargs:
    temperature: 0.0
    
transformers_kwargs:
  call_fn: vlm_load_call
  model_cls: AutoModelForVision2Seq
  model_kwargs:
    pretrained_model_name_or_path: Qwen/Qwen2.5-VL-32B-Instruct
    load_in_4bit: True
    attn_implementation: "flash_attention_2"
    # pretrained_model_name_or_path: Qwen/Qwen2.5-VL-7B-Instruct
  forward_kwargs:
    max_new_tokens: 512
    batch_size: 8
  gpu_ids: [0,1,2]